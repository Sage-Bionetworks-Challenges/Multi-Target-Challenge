Prediction Methods: Our model, which we term [ModelNameHidden], is a combination convolutional-recurrent neural network that predicts the affinity of drug-protein binding using a numerical representation of the molecule via a variational autoencoder framework (R GÃ³mez-Bombarelli et al, 2016) and the amino acid sequence of the protein. [ModelNameHidden] predicts binding of the molecule at each position of the protein sequence and outputs a confidence score for that positional prediction. The overall predicted affinity is calculated from the weighted average (dot product) of the affinity and confidence score. Using this approach, our model is able to infer the binding site of the molecule in addition to its overall affinity for the protein. The protein sequence is featurized first by splitting it into trimers, yielding three different sequences in different frames. We pre-learn an embedding of the protein trimers using the FastText algorithm run over the entire SwissProt database. These trimer embeddings are then fed into a convolutional layer of size 3, which is then fed into a dilated convolution layer of size 3 and dilation 3, which then is fed into another dilated convolution layer of size 3 and dilation 9. These three convolutional layers are then concatenated along with the original trimer embedding to create a multi-scale representation of each protein position. We also concatenate Pfam domain annotations of each sequence position in an embedding layer created by converting the numeric representation of each domain into a dense vector. Further, we then  concatenate the molecule representation by tiling across the full length of the protein sequence. The resultant representation is fed into a Bidirectional Long Term- Short Term (LSTM) recurrent neural network which converts these features into the positional binding and confidence predictions. A final dot product layer summarizes the normalized confidence predictions with the binding positional binding affinity. This approach allows us to use the entire ChEMBL 23 dataset with over 2 million binding affinities to learn one unified binding model that can take as input any protein sequence and SMILES string. We are even able to learn the effect of mutations on binding by feeding in sequence variants as annotated by ChEMBL. We are happy to provide more graphical representations of our model to help visualize the overall architecture. We are also happy to provide the predicted binding sites for the DREAM challenge predictions.


Rationale:
    Why is your approach innovative?: The most novel aspect of our model is that we do not only make one prediction for the overall binding but additionally predict along the entire sequence of the protein. This helps the model to learn an explanatory, mechanistic understanding of binding from sequence features. We also use a hybrid dilated convolutional - LSTM neural network that is better able to learn deep sequence features that are predictive of binding to a given molecular architecture. Our use of an embedded domain representation is also innovative and better helps our model identify the correct binding mechanism. These innovative features all stem from the fundamental design principle of [ModelNameHidden], that binding is not a property of a protein but a property of sub-sequence of the protein. This approach to the machine learning prediction of binding is novel because it avoids the creation of a collapsed (and thereby highly lossy) representation of the protein. We also predict affinity instead of binding as a binary output which is pretty difficult and quite novel for a machine learning model.
    Why will your approach be generalizable?: The first aspect that makes our model generalizable is that the inputs to our model are simply a sequence representation of a protein and a SMILES string, allowing our single model to predict any desired interaction. Our approach also generalizes well because of the sheer amount of data that is fed into our model. We do not train one model per protein but instead use all known binding interactions to learn the general properties of binding. We further use all of SwissProt to learn effective representations of amino acid sequences through FastText algorithm. This allows us to learn sequence similarity of different amino acid trimers without using any binding data. We also use PFAM to annotate domains which are responsible for a significant portion of binding. The molecular representation of the drug is also similarly learned in an unsupervised way from the entirety of ChEMBL through a variational autoencoder. Thus, even before the model is shown any binding data it already has significant amount of prior information that helps it isolate the correct mechanism. We further show this generalizability by splitting our training and testing data by protein instead of by molecule. We evaluated multiple deep learning architectures and the one that we present here is the only one that is capable of performing well in this test. When we correlate experimental replicate log IC50s for the test proteins we observe an 0.80 Pearson rho. Averaging these replicates and correlating them to the model predicted log IC50, we see a 0.76 Pearson rho. This result shows that even when the model has not seen a single binding interaction for a given protein, it is still able to achieve near experimental accuracy for the prediction. Further the model can be shown to be generalizable because it accurately identifies the residues that are responsible for binding, showing that it converges to the true mechanism - even though no mechanistic ground-truth was ever given to the model. For instance, in our prediction of Gleevec binding to ABL1, we can plot the predicted binding affinity along the sequence and show that our model restricts its predictions overwhelmingly to the protein kinase domain of ABL1 and achieves maximal prediction near the 'gatekeeper' Thr315 residue which is considered among the most important for Gleevec binding. Furthermore, because our model learns binding using a multi-scale sequence representation, it will be able to borrow information effectively across proteins to make truly novel and generalizable predictions.


Problem 1:
    - Solution 1:
        ZINC ID: ZINC02093883
        VENDOR ID: MCULE-9576079569
        SMILES string: CC(C)OCCCN1CC(=O)N2[C@H](C1=O)Cc3c4ccccc4[nH]c3[C@@H]2c5ccccc5
        VENDOR NAME: Mcule
        Explanation of chemical novelty: Visibly different, lacks the quinazoline core common to many TKIs
    - Solution 2:
        ZINC ID: ZINC02833706
        VENDOR ID: 7472894
        SMILES string: c1ccc(c(c1)C(=O)[O-])NC(=S)NC(=O)/C=C/c2cccc(c2)[N+](=O)[O-]
        VENDOR NAME: ChemBridge
        Explanation of chemical novelty: Very distinctive from current TKIs
Problem 2:
    - Solution 1:
        ZINC ID: ZINC84724044
        VENDOR ID: M-124001
        SMILES string: C[C@@H]1CC[C@H]2C(=C1)C=C[C@@H]([C@]23CCC(=O)O3)C
        VENDOR NAME: TLC Pharmaceutical Standards
        Explanation of chemical novelty:  Distinctive from traditional kinase inhibitors
    - Solution 2: 
        ZINC ID: ZINC00001266
        VENDOR ID: Y0427
        SMILES string: C[C@@H](C1=[NH+]CCN1)Oc2c(cccc2Cl)Cl
        VENDOR NAME: AK Scientific
        Explanation of chemical novelty: While the compound, lofexidine, is itself well known, the use in this context would be novel
